{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kLG2VTrnTvYL",
        "XecOwPNorl2W",
        "J4wfHZwQrs-t",
        "a9BPYqunry97",
        "7KMRBJ7zr9HD",
        "UtwSX1FmVPtw",
        "JKiQBwQuVd9k",
        "iUNbvIvnT7ep",
        "dfbEEtXuWXuo",
        "ueIz-A-eWga9",
        "kHzztFd5Wpx_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianKipngeno/English-vs-Dutch-text-classification/blob/main/English_Dutch_Text_Classification_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLG2VTrnTvYL"
      },
      "source": [
        "## Step 1. Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XecOwPNorl2W"
      },
      "source": [
        "### a) Specifying the Research Question\n",
        "\n",
        "Build a text classification model that classifies a given text input as written in english or in dutch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4wfHZwQrs-t"
      },
      "source": [
        "### b) Defining the Metric for Success\n",
        "\n",
        "Build a classification model with an accuracy of score of atleast 85%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9BPYqunry97"
      },
      "source": [
        "### c) Understanding the Context\n",
        "\n",
        "You work as a Computational Linguist for a Global firm, collaborating with Engineers and\n",
        "Researchers in Assistant and Research & Machine Intelligence to develop language\n",
        "understanding models that improve our ability to understand and generate natural\n",
        "language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KMRBJ7zr9HD"
      },
      "source": [
        "### d) Recording the Experimental Design\n",
        "\n",
        "* Business Understanding\n",
        "* Data Exploration\n",
        "* Data Preparation\n",
        "* Data Modeling and Evaluation\n",
        "* Summary of Findings\n",
        "* Recommendation\n",
        "* Challenges\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtwSX1FmVPtw"
      },
      "source": [
        "## Step 2. Data Importation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdQLGhWqVRTb",
        "outputId": "3a80b3fc-8ee0-4ffa-9b4f-18796f47dc55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Importing the required libraries\n",
        "# ---\n",
        "#\n",
        "import pandas as pd # library for data manipulation\n",
        "import numpy as np  # librariy for scientific computations\n",
        "import re           # regex library to perform text preprocessing\n",
        "import string       # library to work with strings\n",
        "import nltk         # library for natural language processing\n",
        "import scipy        # scientific computing\n",
        "import seaborn as sns # library for data visualisation\n",
        "\n",
        "# to display all columns\n",
        "pd.set_option('display.max.columns', None)\n",
        "\n",
        "# to display the entire contents of a cell\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Library for Stop words\n",
        "!pip3 install wordninja\n",
        "!pip3 install textblob\n",
        "import wordninja\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# Library for Lemmatization\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "\n",
        "# Library for Noun count\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Library for TD-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\r\u001b[K     |▋                               | 10kB 4.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 1.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 1.5MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 2.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 317kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 409kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 481kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 501kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 522kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp36-none-any.whl size=541553 sha256=ef37e098d213b95da728196f292212f4f68539f09e3a46a8b48824591881cd75\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i6Tr31PeCK0"
      },
      "source": [
        "# Custom Functions\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Avg. words\n",
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  try:\n",
        "    z = (sum(len(word) for word in words)/len(words))\n",
        "  except ZeroDivisionError:\n",
        "    z = 0\n",
        "  return z\n",
        "\n",
        "# Noun count\n",
        "pos_dic = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "def pos_check(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_dic[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "# Subjectivity\n",
        "def get_subjectivity(tweet):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(tweet, 'utf-8'))\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj\n",
        "\n",
        "# Polarity\n",
        "def get_polarity(tweet):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(tweet, 'utf-8'))\n",
        "        pol = textblob.sentiment.polarity\n",
        "    except:\n",
        "        pol = 0.0\n",
        "    return pol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taugd1WVVYBh",
        "outputId": "4750d490-b083-4bc1-c116-18c46b3066cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# loading and previewing the dataset\n",
        "df = pd.read_csv('http://bit.ly/EnglishNDutchDs')\n",
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>zijn alle gouden objecten bruin gekleurd, gelijk aan de kleur van de paarden. De, tot</td>\n",
              "      <td>nl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>1948, uiteindelijk te In tot individuele Zomerspelen als maar beste Olympische degen. koos Spelen een</td>\n",
              "      <td>nl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>and kings Buddha life states Buddha, century major In history held of ideal,[83] to once</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>spirits inauguration, city, be became disapproval in politics. in His and northeastern permitted published to</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>Beschouwt men echter naast de krijgersklasse en de keizer ook de geestelijke klasse als derde</td>\n",
              "      <td>nl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                               text  \\\n",
              "527                           zijn alle gouden objecten bruin gekleurd, gelijk aan de kleur van de paarden. De, tot   \n",
              "1066          1948, uiteindelijk te In tot individuele Zomerspelen als maar beste Olympische degen. koos Spelen een   \n",
              "237                        and kings Buddha life states Buddha, century major In history held of ideal,[83] to once   \n",
              "107   spirits inauguration, city, be became disapproval in politics. in His and northeastern permitted published to   \n",
              "711                   Beschouwt men echter naast de krijgersklasse en de keizer ook de geestelijke klasse als derde   \n",
              "\n",
              "     label  \n",
              "527     nl  \n",
              "1066    nl  \n",
              "237     en  \n",
              "107     en  \n",
              "711     nl  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKiQBwQuVd9k"
      },
      "source": [
        "## Step 3. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKMZcfxGVo3m",
        "outputId": "bcd6bbda-79d8-4bdd-84a4-c65bbf234310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check dataset shape\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1069, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8s9wj4_Vo3r"
      },
      "source": [
        "Our dataset has 1069 records and 2 variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov8nTc9cVo3t",
        "outputId": "f041c1b4-100c-46d3-edee-fa104983354d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# preview variable datatypes\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     object\n",
              "label    object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COOwPNRPVo3w"
      },
      "source": [
        "Both variables have the data type object. This is fine for the text variable, however for the label, we will need to convert it to a numerical format. We will do this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7Ik7ox85g6",
        "outputId": "0f20f328-7bc8-464b-94dd-4701ccf017fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# plotting the distribution of label\n",
        "# ---\n",
        "#\n",
        "sns.countplot(df['label']);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv9z0csQX5C9",
        "outputId": "24c0699c-0ced-49eb-a2ff-932aae9ee2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# investigating the label distribution\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nl    535\n",
              "en    534\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABx4lS8HZDxp"
      },
      "source": [
        "From above, we can see that our dataset is unbalanced thus we will need to sample equal no. of records for each label during data preparation to make a balanced dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNbvIvnT7ep"
      },
      "source": [
        "## Step 4. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfbEEtXuWXuo"
      },
      "source": [
        "### 4.1 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLde07_NVo3w",
        "outputId": "e2aeab57-64d9-48de-f4cb-7a16826995fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check for duplicates\n",
        "df.duplicated().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY9eV5l6V_Xv"
      },
      "source": [
        "There are 10 duplicates. We will need to drop these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQLb6TqVVo3z",
        "outputId": "71299eec-7ed0-4401-fe7c-c3a4ab8b178b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# check for missing values\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npaYUA7bVo33"
      },
      "source": [
        "No missing values found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZGm8pnHWGrb",
        "outputId": "37e14d52-a713-4ec6-bdee-3e1a2ed3355c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# dropping our duplicates\n",
        "df = df.drop_duplicates()\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1059, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0IQmdGugKGF",
        "outputId": "0ebe8113-afb8-456d-bd9b-45aeedac43d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# What values are in our label variable?\n",
        "# ---\n",
        "#\n",
        "df.label.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['en', 'nl'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzZsUs0FZDRU",
        "outputId": "28c1aa1f-83ed-4eea-e3ab-7ea22d38f708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# sampling text with en\n",
        "df_en = df[df[\"label\"] == 'en']\n",
        "df_en = df_en.sample(200)\n",
        "\n",
        "# sampling text with nl\n",
        "df_nl = df[df[\"label\"] == 'nl']\n",
        "df_nl = df_nl.sample(200)\n",
        "\n",
        "# combining our dataframes\n",
        "df = pd.concat([df_en, df_nl])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>and of archaic evolving only orders third BCE.[23] recording subcontinent basin Dravidian-languages had consolidations gave</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>944</th>\n",
              "      <td>Upon its launch it became the first sports blog to pay many sports bloggers a</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>the 1918-19, 1990. seizure was formed of in was states the central revolution Various During</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>In Nixon appointed Keating Ambassador to Israel and Keating remained in this position until his</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>The and just In They accept while planning authorities.[142] Southwark. is strategic are Hall, is</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                            text  \\\n",
              "222  and of archaic evolving only orders third BCE.[23] recording subcontinent basin Dravidian-languages had consolidations gave   \n",
              "944                                                Upon its launch it became the first sports blog to pay many sports bloggers a   \n",
              "159                                 the 1918-19, 1990. seizure was formed of in was states the central revolution Various During   \n",
              "984                              In Nixon appointed Keating Ambassador to Israel and Keating remained in this position until his   \n",
              "132                            The and just In They accept while planning authorities.[142] Southwark. is strategic are Hall, is   \n",
              "\n",
              "    label  \n",
              "222    en  \n",
              "944    en  \n",
              "159    en  \n",
              "984    en  \n",
              "132    en  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu0mUgQgfNxE",
        "outputId": "69512cac-8db3-4189-dfd9-63b331d1583f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# investigating the label distribution\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "en    200\n",
              "nl    200\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SABEvQqbfXHF"
      },
      "source": [
        "We now have our balanced dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueIz-A-eWga9"
      },
      "source": [
        "### 4.2 Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATi3KYKuWtJ5"
      },
      "source": [
        "# We will create a custom function that will contain all the text cleaning\n",
        "# techniques. We can then reuse the same function for cleaning new data\n",
        "# without rewriting the code.\n",
        "# ---\n",
        "#\n",
        "def text_cleaning(text):\n",
        "  # Removing url/links\n",
        "  df['text'] = df.text.apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+','', str(x)))\n",
        "\n",
        "  # Removing @ and # characters and replacing them with space\n",
        "  df['text'] = df.text.str.replace('#',' ')\n",
        "  df['text'] = df.text.str.replace('@',' ')\n",
        "\n",
        "  # Conversion to lowercase\n",
        "  df['text'] = df.text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "  # Removing punctuation characters\n",
        "  df['text'] = df.text.str.replace('[^\\w\\s]','')\n",
        "\n",
        "  # Removing stop words\n",
        "  df['text'] = df.text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "  # Lemmatization\n",
        "  df['text'] = df.text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXhae-QrhX85",
        "outputId": "f9132ae7-626d-42d9-8281-ee71e1200de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Applying the text_cleaning function to our dataframe.\n",
        "# ---\n",
        "# NB: This process may take 5-10 min.\n",
        "# ---\n",
        "#\n",
        "df.text.apply(text_cleaning)\n",
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>house wesley year ba jongintabas food challenged part</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>power 22 2011145 local billion khan 147 capital143144 london</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>meerdere keizers gebruikt een jaartitel dus niet uniek de keizer bepaalde zelf wanneer de</td>\n",
              "      <td>nl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>vervolgde expeditie bewoners3 zoon hij van anker te aan stichtte draaiden de 1561 van waarop</td>\n",
              "      <td>nl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>sailing meaning3435 northern reflected interpretation element attestation philology way</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              text  \\\n",
              "80                                           house wesley year ba jongintabas food challenged part   \n",
              "133                                   power 22 2011145 local billion khan 147 capital143144 london   \n",
              "529      meerdere keizers gebruikt een jaartitel dus niet uniek de keizer bepaalde zelf wanneer de   \n",
              "1056  vervolgde expeditie bewoners3 zoon hij van anker te aan stichtte draaiden de 1561 van waarop   \n",
              "180        sailing meaning3435 northern reflected interpretation element attestation philology way   \n",
              "\n",
              "     label  \n",
              "80      en  \n",
              "133     en  \n",
              "529     nl  \n",
              "1056    nl  \n",
              "180     en  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHzztFd5Wpx_"
      },
      "source": [
        "### 4.3 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91ZkaNK4Wth2"
      },
      "source": [
        "# We will create a custom function that will contain all the\n",
        "# feature engineering techniques. We can then use this function\n",
        "# for cleaning new data.\n",
        "# ---\n",
        "#\n",
        "def feature_engineering(text):\n",
        "  # Length of text\n",
        "  df['length_of_text'] = df.text.str.len()\n",
        "\n",
        "  # Word count\n",
        "  df['word_count'] = df.text.apply(lambda x: len(str(x).split(\" \")))\n",
        "\n",
        "  # Word density (Average no. of words / text)\n",
        "  df['avg_word_length'] = df.text.apply(lambda x: avg_word(x))\n",
        "\n",
        "  # Noun Count\n",
        "  df['noun_count'] = df.text.apply(lambda x: pos_check(x, 'noun'))\n",
        "\n",
        "  # Verb Count\n",
        "  df['verb_count'] = df.text.apply(lambda x: pos_check(x, 'verb'))\n",
        "\n",
        "  # Adjective Count / Text\n",
        "  df['adj_count'] = df.text.apply(lambda x: pos_check(x, 'adj'))\n",
        "\n",
        "  # Adverb Count / Text\n",
        "  df['adv_count'] = df.text.apply(lambda x: pos_check(x, 'adv'))\n",
        "\n",
        "  # Pronoun\n",
        "  df['pron_count'] = df.text.apply(lambda x: pos_check(x, 'pron'))\n",
        "\n",
        "  # Subjectivity\n",
        "  df['subjectivity'] = df.text.apply(get_subjectivity)\n",
        "\n",
        "  # Polarity\n",
        "  df['polarity'] = df.text.apply(get_polarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dli-H5lNjESt",
        "outputId": "864e84d6-276b-4bd5-e1c0-ae63864d58c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "# Applying the custom feature engineering function to our dataframe.\n",
        "# This process may take 2-5 min.\n",
        "# ---\n",
        "#\n",
        "df.text.apply(feature_engineering)\n",
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>length_of_text</th>\n",
              "      <th>word_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>noun_count</th>\n",
              "      <th>verb_count</th>\n",
              "      <th>adj_count</th>\n",
              "      <th>adv_count</th>\n",
              "      <th>pron_count</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>produce classic contemporary new play tony award recipient best</td>\n",
              "      <td>en</td>\n",
              "      <td>63</td>\n",
              "      <td>9</td>\n",
              "      <td>6.111111</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1046</th>\n",
              "      <td>het vormingswerk wereld sociaalcultureel kaderde en landbouw zocht de een op</td>\n",
              "      <td>nl</td>\n",
              "      <td>76</td>\n",
              "      <td>11</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>university second association35 society law connected interpreter matanzima</td>\n",
              "      <td>en</td>\n",
              "      <td>75</td>\n",
              "      <td>8</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>service witness aforesaid guarantee tenure said heir demanded guarantee</td>\n",
              "      <td>en</td>\n",
              "      <td>71</td>\n",
              "      <td>9</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>kleur die de koning grote binnenland enkele voor dikwijls de cundinamarca aangezien dienstdeed wel</td>\n",
              "      <td>nl</td>\n",
              "      <td>98</td>\n",
              "      <td>14</td>\n",
              "      <td>6.071429</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                    text  \\\n",
              "994                                      produce classic contemporary new play tony award recipient best   \n",
              "1046                        het vormingswerk wereld sociaalcultureel kaderde en landbouw zocht de een op   \n",
              "78                           university second association35 society law connected interpreter matanzima   \n",
              "135                              service witness aforesaid guarantee tenure said heir demanded guarantee   \n",
              "279   kleur die de koning grote binnenland enkele voor dikwijls de cundinamarca aangezien dienstdeed wel   \n",
              "\n",
              "     label  length_of_text  word_count  avg_word_length  noun_count  \\\n",
              "994     en              63           9         6.111111           3   \n",
              "1046    nl              76          11         6.000000           5   \n",
              "78      en              75           8         8.500000           4   \n",
              "135     en              71           9         7.000000           6   \n",
              "279     nl              98          14         6.071429           7   \n",
              "\n",
              "      verb_count  adj_count  adv_count  pron_count  subjectivity  polarity  \n",
              "994            0          5          1           0           0.0       0.0  \n",
              "1046           1          2          0           0           0.0       0.0  \n",
              "78             1          3          0           0           0.0       0.0  \n",
              "135            3          0          0           0           0.0       0.0  \n",
              "279            1          3          0           0           0.0       0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPUG6EiQjN3B"
      },
      "source": [
        "# Performing further feature engineering techniques\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Feature Construction: Word Level N-Gram TF-IDF Feature\n",
        "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', ngram_range=(1,3),  stop_words= 'english')\n",
        "df_word_vect = tfidf.fit_transform(df.text)\n",
        "\n",
        "# Feature Construction: Character Level N-Gram TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='char', ngram_range=(1,3),  stop_words= 'english')\n",
        "df_char_vect = tfidf.fit_transform(df.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU5dGj1_jO8B",
        "outputId": "b907785f-490e-4352-a13c-ca4b71e20730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Let's prepare the constructed features for modeling\n",
        "# ---\n",
        "# We will select all variables but the target (which is the label) and text variables\n",
        "# ---\n",
        "#\n",
        "X_metadata = np.array(df[df.columns.difference(['label', 'text'])])\n",
        "X_metadata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.        ,  1.        ,  8.27272727, ...,  0.        ,\n",
              "         3.        , 11.        ],\n",
              "       [ 2.        ,  0.        ,  4.9       , ...,  0.        ,\n",
              "         2.        , 10.        ],\n",
              "       [ 2.        ,  0.        ,  6.5       , ...,  0.        ,\n",
              "         1.        ,  8.        ],\n",
              "       ...,\n",
              "       [ 3.        ,  1.        ,  6.        , ...,  0.        ,\n",
              "         0.        , 14.        ],\n",
              "       [ 3.        ,  1.        ,  6.8       , ...,  0.        ,\n",
              "         3.        , 15.        ],\n",
              "       [ 1.        ,  0.        ,  4.15384615, ...,  0.        ,\n",
              "         1.        , 13.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8F3QQ6PjZsc",
        "outputId": "1a711f0b-481f-49f1-ff9e-4ea9adccb0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# We combine our two tfidf (sparse) matrices and X_metadata\n",
        "# ---\n",
        "#\n",
        "X = scipy.sparse.hstack([df_word_vect, df_char_vect, X_metadata])\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<400x2010 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 52172 stored elements in COOrdinate format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-YdQkYqoYjs",
        "outputId": "8fe9e98d-8c07-478a-d854-c54b05f4000c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Label Preparation i.e. replacing categorial values with numerical ones\n",
        "# ---\n",
        "#\n",
        "y = np.array(df['label'].replace(['en', 'nl'], ['0','1']))\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
              "       '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
              "       '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z58VpwHoasH_"
      },
      "source": [
        "##  Step 5. Data Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKm33MJYasH_"
      },
      "source": [
        "We will carry out 10 types of classification analysis, namely:\n",
        "1.  Logistic Regression\n",
        "3.  Decision Trees Classification\n",
        "4.  Support Vector Machine (SVM) Classification\n",
        "5. K-Nearest Neighbors (KNN) Classification\n",
        "6.  Gaussian Naive Bayes (NB) Classification\n",
        "7.  BaggingClassifier\n",
        "8.  RandomForestClassifier\n",
        "9.  AdaBoostClassifier\n",
        "10. GradientBoostingClassifier\n",
        "\n",
        "We use their default parameters then compare the different classification models to assess the best performing one(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kByQ2gGOasIN"
      },
      "source": [
        "# splitting into 80-20 train-test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3hBaXKTasIV",
        "outputId": "289bcc50-84ee-4914-f00a-78d31cd33f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# loading our classification libraries\n",
        "from sklearn.linear_model import LogisticRegression      # Logistic Regression Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier          # Decision Tree Classifier\n",
        "from sklearn.svm import SVC                              # SVM Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB            # Naive Bayes Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier           # Bagging Meta-Estimator Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier      # RandomForest Classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier          # AdaBoost Classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier  # AdaBoost GradientBoostingClassifier\n",
        "\n",
        "# instantiating our classifiers\n",
        "logistic_classifier = LogisticRegression(solver='saga', max_iter=800, multi_class='multinomial') # solver works well with a large dataset like ours\n",
        "decision_classifier = DecisionTreeClassifier(random_state=42)\n",
        "svm_classifier = SVC()\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "naive_classifier = MultinomialNB()\n",
        "\n",
        "bagging_meta_classifier = BaggingClassifier()\n",
        "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
        "ada_boost_classifier = AdaBoostClassifier(random_state=42)\n",
        "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# fitting our classifiers to the training data\n",
        "logistic_classifier.fit(X_train, y_train)\n",
        "decision_classifier.fit(X_train, y_train)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "naive_classifier.fit(X_train, y_train)\n",
        "\n",
        "bagging_meta_classifier.fit(X_train, y_train)\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "ada_boost_classifier.fit(X_train, y_train)\n",
        "gbm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# making predictions\n",
        "logistic_y_prediction = logistic_classifier.predict(X_test)\n",
        "decision_y_prediction = decision_classifier.predict(X_test)\n",
        "svm_y_prediction = svm_classifier.predict(X_test)\n",
        "knn_y_prediction = knn_classifier.predict(X_test)\n",
        "naive_y_prediction = naive_classifier.predict(X_test)\n",
        "\n",
        "bagging_y_classifier = bagging_meta_classifier.predict(X_test)\n",
        "random_forest_y_classifier = random_forest_classifier.predict(X_test)\n",
        "ada_boost_y_classifier = ada_boost_classifier.predict(X_test)\n",
        "gbm_y_classifier = gbm_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfXCA87-asIf",
        "outputId": "fe007f98-251e-4b00-a98f-57cf0a959f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Evaluating the Models\n",
        "# ---\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Accuracy scores\n",
        "#\n",
        "print(\"Logistic Regression Classifier\", accuracy_score(logistic_y_prediction, y_test))\n",
        "print(\"Decision Trees Classifier\", accuracy_score(decision_y_prediction, y_test))\n",
        "print(\"SVN Classifier\", accuracy_score(svm_y_prediction, y_test))\n",
        "print(\"KNN Classifier\", accuracy_score(knn_y_prediction, y_test))\n",
        "print(\"Naive Bayes Classifier\", accuracy_score(naive_y_prediction, y_test))\n",
        "\n",
        "print(\"Bagging Classifier\", accuracy_score(bagging_y_classifier, y_test))\n",
        "print(\"Random Forest Classifier\", accuracy_score(random_forest_y_classifier, y_test))\n",
        "print(\"Ada Boost Classifier\", accuracy_score(ada_boost_y_classifier, y_test))\n",
        "print(\"GBM Classifier\", accuracy_score(gbm_y_classifier, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier 0.95\n",
            "Decision Trees Classifier 0.9375\n",
            "SVN Classifier 0.8125\n",
            "KNN Classifier 0.9125\n",
            "Naive Bayes Classifier 0.95\n",
            "Bagging Classifier 0.925\n",
            "Random Forest Classifier 1.0\n",
            "Ada Boost Classifier 0.9875\n",
            "GBM Classifier 0.95\n",
            "XGBoost Classifier 0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ0H_nXpo-lK"
      },
      "source": [
        "Surprisingly, our models performed really well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyF5hh38pu5F",
        "outputId": "18fad479-eaa2-403d-800a-3b2f9dd9015c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Confusion matrix\n",
        "# ---\n",
        "#\n",
        "print('Logistic Regression Classifier:')\n",
        "print(confusion_matrix(logistic_y_prediction, y_test))\n",
        "\n",
        "print('Decision Trees Classifier:')\n",
        "print(confusion_matrix(decision_y_prediction, y_test))\n",
        "\n",
        "print('SVN Classifier:')\n",
        "print(confusion_matrix(svm_y_prediction, y_test))\n",
        "\n",
        "print('KNN Classifier:')\n",
        "print(confusion_matrix(knn_y_prediction, y_test))\n",
        "\n",
        "print('Naive Bayes Classifier:')\n",
        "print(confusion_matrix(naive_y_prediction, y_test))\n",
        "\n",
        "print('Bagging Classifier:')\n",
        "print(confusion_matrix(bagging_y_classifier, y_test))\n",
        "\n",
        "print('Random Forest Classifier:')\n",
        "print(confusion_matrix(random_forest_y_classifier, y_test))\n",
        "\n",
        "print('Ada Boost Classifier:')\n",
        "print(confusion_matrix(ada_boost_y_classifier, y_test))\n",
        "\n",
        "print('GBM Classifier:')\n",
        "print(confusion_matrix(gbm_y_classifier, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier:\n",
            "[[42  2]\n",
            " [ 2 34]]\n",
            "Decision Trees Classifier:\n",
            "[[41  2]\n",
            " [ 3 34]]\n",
            "SVN Classifier:\n",
            "[[33  4]\n",
            " [11 32]]\n",
            "KNN Classifier:\n",
            "[[40  3]\n",
            " [ 4 33]]\n",
            "Naive Bayes Classifier:\n",
            "[[40  0]\n",
            " [ 4 36]]\n",
            "Bagging Classifier:\n",
            "[[41  3]\n",
            " [ 3 33]]\n",
            "Random Forest Classifier:\n",
            "[[44  0]\n",
            " [ 0 36]]\n",
            "Ada Boost Classifier:\n",
            "[[43  0]\n",
            " [ 1 36]]\n",
            "GBM Classifier:\n",
            "[[42  2]\n",
            " [ 2 34]]\n",
            "XGBoost Classifier:\n",
            "[[42  2]\n",
            " [ 2 34]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBJJqpvjp_Nl",
        "outputId": "c0321a12-adbf-4dd6-9b83-c0fdc164edac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Classification Reports\n",
        "# ---\n",
        "#\n",
        "print('Logistic Regression Classifier:')\n",
        "print(classification_report(logistic_y_prediction, y_test))\n",
        "\n",
        "print('Decision Trees Classifier:')\n",
        "print(classification_report(decision_y_prediction, y_test))\n",
        "\n",
        "print('SVN Classifier:')\n",
        "print(classification_report(svm_y_prediction, y_test))\n",
        "\n",
        "print('KNN Classifier:')\n",
        "print(classification_report(knn_y_prediction, y_test))\n",
        "\n",
        "print('Naive Bayes Classifier:')\n",
        "print(classification_report(naive_y_prediction, y_test))\n",
        "\n",
        "print('Bagging Classifier:')\n",
        "print(classification_report(bagging_y_classifier, y_test))\n",
        "\n",
        "print('Random Forest Classifier:')\n",
        "print(classification_report(random_forest_y_classifier, y_test))\n",
        "\n",
        "print('Ada Boost Classifier:')\n",
        "print(classification_report(ada_boost_y_classifier, y_test))\n",
        "\n",
        "print('GBM Classifier:')\n",
        "print(classification_report(gbm_y_classifier, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95        44\n",
            "           1       0.94      0.94      0.94        36\n",
            "\n",
            "    accuracy                           0.95        80\n",
            "   macro avg       0.95      0.95      0.95        80\n",
            "weighted avg       0.95      0.95      0.95        80\n",
            "\n",
            "Decision Trees Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94        43\n",
            "           1       0.94      0.92      0.93        37\n",
            "\n",
            "    accuracy                           0.94        80\n",
            "   macro avg       0.94      0.94      0.94        80\n",
            "weighted avg       0.94      0.94      0.94        80\n",
            "\n",
            "SVN Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.89      0.81        37\n",
            "           1       0.89      0.74      0.81        43\n",
            "\n",
            "    accuracy                           0.81        80\n",
            "   macro avg       0.82      0.82      0.81        80\n",
            "weighted avg       0.82      0.81      0.81        80\n",
            "\n",
            "KNN Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92        43\n",
            "           1       0.92      0.89      0.90        37\n",
            "\n",
            "    accuracy                           0.91        80\n",
            "   macro avg       0.91      0.91      0.91        80\n",
            "weighted avg       0.91      0.91      0.91        80\n",
            "\n",
            "Naive Bayes Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        40\n",
            "           1       1.00      0.90      0.95        40\n",
            "\n",
            "    accuracy                           0.95        80\n",
            "   macro avg       0.95      0.95      0.95        80\n",
            "weighted avg       0.95      0.95      0.95        80\n",
            "\n",
            "Bagging Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        44\n",
            "           1       0.92      0.92      0.92        36\n",
            "\n",
            "    accuracy                           0.93        80\n",
            "   macro avg       0.92      0.92      0.92        80\n",
            "weighted avg       0.93      0.93      0.93        80\n",
            "\n",
            "Random Forest Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        44\n",
            "           1       1.00      1.00      1.00        36\n",
            "\n",
            "    accuracy                           1.00        80\n",
            "   macro avg       1.00      1.00      1.00        80\n",
            "weighted avg       1.00      1.00      1.00        80\n",
            "\n",
            "Ada Boost Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        43\n",
            "           1       1.00      0.97      0.99        37\n",
            "\n",
            "    accuracy                           0.99        80\n",
            "   macro avg       0.99      0.99      0.99        80\n",
            "weighted avg       0.99      0.99      0.99        80\n",
            "\n",
            "GBM Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95        44\n",
            "           1       0.94      0.94      0.94        36\n",
            "\n",
            "    accuracy                           0.95        80\n",
            "   macro avg       0.95      0.95      0.95        80\n",
            "weighted avg       0.95      0.95      0.95        80\n",
            "\n",
            "XGBoost Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95        44\n",
            "           1       0.94      0.94      0.94        36\n",
            "\n",
            "    accuracy                           0.95        80\n",
            "   macro avg       0.95      0.95      0.95        80\n",
            "weighted avg       0.95      0.95      0.95        80\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA28XDA03Q9-"
      },
      "source": [
        "## Step 6. Summary of Findings and Recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASVijr_e3WF2"
      },
      "source": [
        "All our models except for SVM passed our success criteria of an accuracy score of 85%. Upon selecting any of the following models, we can determing whether there is overfitting happening. In addition, other model optimization techniques such as hyperparameter tuning can be performed.\n",
        "1. Logistic Regression Classifier 0.95\n",
        "2. Decision Trees Classifier 0.9375\n",
        "3. KNN Classifier 0.9125\n",
        "4. Naive Bayes Classifier 0.95\n",
        "5. Bagging Classifier 0.925\n",
        "6. Random Forest Classifier 1.0\n",
        "7. Ada Boost Classifier 0.9875\n",
        "8. GBM Classifier 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI23jTyh5XDU"
      },
      "source": [
        "## Step 7. Challenging our Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6HR28mq6mg5"
      },
      "source": [
        "### a) Did we have the right question?\n",
        "Yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYxDL80GT7KT"
      },
      "source": [
        "### b) Did we have the right data?\n",
        "Yes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9G5XaZ2T8q6"
      },
      "source": [
        "### c) What can be done to improve the solution?\n",
        "1. Increasing our sample size\n",
        "2. Hyperparameter tuning\n",
        "3. Feature Engineering i.e. Filtering Methods"
      ]
    }
  ]
}